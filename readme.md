# ğŸ¥ Hospital Readmission Analysis & ETL Pipeline (PostgreSQL + Python)

This project is a data analytics case study focused on hospital readmissions. It utilizes PostgreSQL as the OLTP database and Python for ETL (Extract, Transform, Load), data processing, exploratory data analysis (EDA), and feature engineering. The primary goal is to analyze factors contributing to hospital readmissions and prepare a dataset suitable for future predictive modeling.

---

## ğŸ“Œ Project Objectives

-   **Data Modeling:** Design and implement a relational data model for hospital admissions in PostgreSQL.
-   **ETL Pipeline:** Develop a robust ETL pipeline using Python to extract data from PostgreSQL, transform it for analysis, and load it into various formats (CSV, database tables).
-   **Data Enrichment & Feature Engineering:** Create new, insightful features from the existing data to better understand readmission patterns.
-   **Exploratory Data Analysis (EDA):** Perform comprehensive EDA to identify trends, patterns, and correlations related to hospital readmissions.
-   **Visualization:** Generate clear and informative visualizations to communicate findings from the EDA.
-   **Code Restructuring & Documentation:** Refactor the codebase for improved modularity, readability, and maintainability, accompanied by thorough documentation.

---

## ğŸ“‚ Project Structure

The project is organized into the following directories:

```
Hospital_Readmission/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .vscode/                    # VSCode specific settings
â”‚   â””â”€â”€ settings.json
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ connect_pg.py           # Script for PostgreSQL database connection
â”‚   â””â”€â”€ connect_pg.py.template  # Template for connect_pg.py
â”œâ”€â”€ data/                       # Data files
â”‚   â”œâ”€â”€ processed/              # Processed and enriched data files
â”‚   â”‚   â”œâ”€â”€ admission_enriched.csv
â”‚   â”‚   â””â”€â”€ transformed_admissions.csv
â”‚   â””â”€â”€ features.csv            # Engineered features for modeling
â”œâ”€â”€ docs/                       # Project documentation
â”‚   â”œâ”€â”€ Hospital_Readmission.docx # Original project documentation (if any)
â”‚   â”œâ”€â”€ data_dictionary.md      # Details of data schemas and fields
â”‚   â””â”€â”€ project_overview.md     # In-depth project description
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for analysis
â”‚   â””â”€â”€ eda_analysis.ipynb      # Interactive EDA notebook
â”œâ”€â”€ reports/                    # Reports and generated figures
â”‚   â”œâ”€â”€ figures/                # Saved plots and visualizations
â”‚   â”‚   â”œâ”€â”€ eda_admissions_by_month.png
â”‚   â”‚   â”œâ”€â”€ eda_los_vs_readmission.png
â”‚   â”‚   â”œâ”€â”€ eda_readmission_by_age_group.png
â”‚   â”‚   â”œâ”€â”€ eda_readmission_by_gender.png
â”‚   â”‚   â””â”€â”€ eda_readmission_by_specialty.png
â”‚   â””â”€â”€ analysis_report.md      # Summary of analysis and findings
â”œâ”€â”€ src/                        # Source code for the project
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_extraction/        # Scripts for extracting data
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ extract.py
â”‚   â”œâ”€â”€ data_processing/        # Scripts for transforming and cleaning data
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â””â”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ database/               # Database related scripts and utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ hospital_readmission.sql # SQL script for DDL and DML
â”‚   â”‚   â””â”€â”€ db_utils.py           # Database utility functions
â”‚   â”œâ”€â”€ etl/                    # ETL pipeline scripts
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ etl_dimensions.py
â”‚   â”‚   â”œâ”€â”€ etl_facts.py
â”‚   â”‚   â””â”€â”€ etl_main_pipeline.py  # Main orchestrator for ETL
â”‚   â”œâ”€â”€ analysis/               # Scripts for data analysis and enrichment
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_enrichment.py
â”‚   â”‚   â””â”€â”€ exploratory_analysis.py
â”‚   â”œâ”€â”€ loading/                # Scripts for loading data to destinations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ load.py
â”‚   â””â”€â”€ utils/                  # Utility functions and helpers
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ tests/                      # Test scripts for the project
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data_processing.py
â”‚   â””â”€â”€ test_etl.py
â”œâ”€â”€ main.py                       # Main script to run pipelines/analyses
â”œâ”€â”€ requirements.txt              # Python package dependencies
â””â”€â”€ README.md                     # This file: Project overview and setup
```

---

## ğŸ“Š Tech Stack

-   **Python 3.x**
    -   `pandas`: For data manipulation and analysis.
    -   `SQLAlchemy`: For ORM and database interaction with PostgreSQL.
    -   `psycopg2-binary`: PostgreSQL adapter for Python.
    -   `matplotlib` & `seaborn`: For data visualization.
    -   `python-dotenv` (recommended): For managing environment variables.
-   **PostgreSQL**: Relational database management system.
-   **Jupyter Notebooks**: For interactive data exploration.
-   **Git**: For version control.

---

## ğŸš€ Features

-   **Modular ETL Pipeline:** Separate modules for extraction, transformation, and loading.
-   **Data Enrichment:** SQL-based data enrichment joining multiple tables.
-   **Feature Engineering:** Creation of new features like length of stay groups, admission month, and readmission labels.
-   **Comprehensive EDA:** Visualizations and statistical summaries of readmission trends by various factors (age, gender, specialty, length of stay).
-   **Structured Project Layout:** Organized folder structure for better maintainability and scalability.
-   **Configuration Management:** Centralized database connection configuration.

---

## ğŸ› ï¸ Project Setup

### 1. Clone the Repository

```bash
git clone <repository_url> # Replace <repository_url> with the actual URL
cd Hospital_Readmission
```

### 2. Create and Activate a Virtual Environment (Recommended)

```bash
python -m venv venv
# On Windows
venv\Scripts\activate
# On macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies

Ensure you have Python 3.x installed. Then, install the required packages:

```bash
pip install -r requirements.txt
```

### 4. Setup the PostgreSQL Database

-   Ensure you have PostgreSQL installed and running.
-   Create a new database (e.g., `hospital_data`).
-   Execute the SQL script `src/database/hospital_readmission.sql` to create the necessary tables and populate initial data. You can use tools like `psql` or PgAdmin.

    ```bash
    psql -U your_username -d hospital_data -f src/database/hospital_readmission.sql
    ```

### 5. Configure Database Connection

-   Navigate to the `config/` directory.
-   Rename `connect_pg.py.template` to `connect_pg.py`.
-   Open `connect_pg.py` and update the database connection details (username, password, host, port, database name). **It is highly recommended to use environment variables for sensitive credentials instead of hardcoding them.**

    Example using `python-dotenv` (add `python-dotenv` to `requirements.txt`):

    Create a `.env` file in the project root with your credentials:
    ```env
    DB_USERNAME=your_postgres_username
    DB_PASSWORD=your_postgres_password
    DB_HOST=localhost
    DB_PORT=5432
    DB_NAME=hospital_data
    ```

    Modify `config/connect_pg.py` to load these variables:
    ```python
    import os
    from sqlalchemy import create_engine
    from dotenv import load_dotenv

    load_dotenv() # Load variables from .env file

    username = os.getenv("DB_USERNAME")
    password = os.getenv("DB_PASSWORD")
    host = os.getenv("DB_HOST")
    port = os.getenv("DB_PORT")
    database = os.getenv("DB_NAME")

    if not all([username, password, host, port, database]):
        raise ValueError("One or more database connection environment variables are not set.")

    # SQLAlchemy engine
    engine = create_engine(f"postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}")
    ```

---

## âš™ï¸ Running the Project

The main entry point for running the ETL pipeline and analyses is `main.py`.

### Running the Full ETL Pipeline and Analysis

```bash
python main.py
```

This script will typically orchestrate the following steps:
1.  Extract data from the PostgreSQL database.
2.  Transform and clean the extracted data.
3.  Perform feature engineering.
4.  Load the processed data to CSV files and/or new database tables.
5.  Execute data enrichment queries.
6.  Generate EDA plots (saved in `reports/figures/`).

### Running Individual Components

You can also run individual scripts if needed, but ensure dependencies and data flow are managed correctly. Refer to the `main.py` script to understand the sequence of operations.

### Exploratory Data Analysis (Notebook)

For interactive EDA, you can use the Jupyter Notebook:

```bash
jupyter notebook notebooks/eda_analysis.ipynb
```

---

## ğŸ“ Documentation

-   **This `README.md`**: Provides an overview, setup, and execution instructions.
-   **`docs/data_dictionary.md`**: Contains descriptions of database tables and CSV file columns.
-   **`docs/project_overview.md`**: Offers a more detailed explanation of the project, its components, and methodologies.
-   **Code Comments & Docstrings**: The source code in `src/` is commented, and functions/modules include docstrings explaining their purpose and usage.

---

## ğŸ§ª Testing

Unit tests are located in the `tests/` directory. To run tests (assuming `pytest` is installed and configured):

```bash
pytest
```

---

## ğŸ¤ Contributing

(Optional: Add guidelines for contributing if this is an open project.)

---

## ğŸ“œ License

(Optional: Specify the project license, e.g., MIT, Apache 2.0.)

